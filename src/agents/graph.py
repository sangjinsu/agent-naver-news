"""
LangGraph 0.6+ ê¸°ë°˜ ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ êµ¬í˜„

ì´ ëª¨ë“ˆì€ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ë¶€í„° ìš”ì•½ê¹Œì§€ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ 
LangGraph StateGraphë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict, Optional

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
# SQLite ì²´í¬í¬ì¸í„°ëŠ” ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©
try:
    from langgraph.checkpoint.sqlite import SqliteSaver
except ImportError:
    SqliteSaver = None

from ..models.schemas import NewsAgentState, create_initial_state, DEFAULT_CATEGORIES
from ..nodes.scraper import scraper_node
from ..nodes.summarizer import summarizer_node


# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


class GraphError(Exception):
    """ê·¸ë˜í”„ ì‹¤í–‰ ê´€ë ¨ ì˜ˆì™¸"""
    pass


def should_retry_scraping(state: NewsAgentState) -> str:
    """
    ìŠ¤í¬ë˜í•‘ ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •í•˜ëŠ” ì¡°ê±´ë¶€ í•¨ìˆ˜
    
    Args:
        state: í˜„ì¬ ìƒíƒœ
        
    Returns:
        ë‹¤ìŒ ë…¸ë“œëª… ("scraper" ë˜ëŠ” "summarizer")
    """
    # ìŠ¤í¬ë˜í•‘ëœ ê¸°ì‚¬ê°€ ì—†ê³  ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì¬ì‹œë„
    total_articles = state.get("total_articles_scraped", 0)
    errors = state.get("errors", [])
    
    # ì¬ì‹œë„ ì¡°ê±´: ê¸°ì‚¬ê°€ 0ê°œì´ê³  ì—ëŸ¬ê°€ ìˆëŠ” ê²½ìš°
    if total_articles == 0 and errors:
        # ì´ë¯¸ ì¬ì‹œë„í–ˆëŠ”ì§€ í™•ì¸ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        scraping_retries = state.get("scraping_retries", 0)
        if scraping_retries < 2:  # ìµœëŒ€ 2íšŒ ì¬ì‹œë„
            logger.warning(f"ìŠ¤í¬ë˜í•‘ ì¬ì‹œë„ ({scraping_retries + 1}/2)")
            return "retry_scraper"
    
    return "summarizer"


def should_retry_summarization(state: NewsAgentState) -> str:
    """
    ìš”ì•½ ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •í•˜ëŠ” ì¡°ê±´ë¶€ í•¨ìˆ˜
    
    Args:
        state: í˜„ì¬ ìƒíƒœ
        
    Returns:
        ë‹¤ìŒ ë…¸ë“œëª… ("summarizer" ë˜ëŠ” "formatter")
    """
    summaries = state.get("summaries", [])
    
    # ìš”ì•½ì´ ì—†ê±°ë‚˜ ëª¨ë“  ìš”ì•½ì´ ì‹¤íŒ¨í•œ ê²½ìš°
    if not summaries or all(not s.get("success", False) for s in summaries):
        # ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸
        summarization_retries = state.get("summarization_retries", 0)
        if summarization_retries < 1:  # ìµœëŒ€ 1íšŒ ì¬ì‹œë„
            logger.warning(f"ìš”ì•½ ì¬ì‹œë„ ({summarization_retries + 1}/1)")
            return "retry_summarizer"
    
    return "formatter"


async def retry_scraper_node(state: NewsAgentState) -> NewsAgentState:
    """
    ì¬ì‹œë„ìš© ìŠ¤í¬ë˜í¼ ë…¸ë“œ
    
    Args:
        state: í˜„ì¬ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    logger.info("ìŠ¤í¬ë˜í•‘ ì¬ì‹œë„ ì‹¤í–‰")
    
    # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
    updated_state = state.copy()
    updated_state["scraping_retries"] = state.get("scraping_retries", 0) + 1
    
    # ì´ì „ ì—ëŸ¬ ì •ë¦¬
    updated_state["errors"] = []
    
    # ìŠ¤í¬ë˜í•‘ ì¬ì‹¤í–‰
    result = await scraper_node(updated_state)
    return result


async def retry_summarizer_node(state: NewsAgentState) -> NewsAgentState:
    """
    ì¬ì‹œë„ìš© ìš”ì•½ ë…¸ë“œ
    
    Args:
        state: í˜„ì¬ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    logger.info("ìš”ì•½ ì¬ì‹œë„ ì‹¤í–‰")
    
    # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
    updated_state = state.copy()
    updated_state["summarization_retries"] = state.get("summarization_retries", 0) + 1
    
    # ìš”ì•½ ì¬ì‹¤í–‰
    result = await summarizer_node(updated_state)
    return result


async def formatter_node(state: NewsAgentState) -> NewsAgentState:
    """
    ë§ˆí¬ë‹¤ìš´ í¬ë§·í„° ë…¸ë“œ (ì„ì‹œ êµ¬í˜„)
    
    Args:
        state: í˜„ì¬ ìƒíƒœ
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    import time
    
    start_time = time.time()
    logger.info("í¬ë§·í„° ë…¸ë“œ ì‹¤í–‰ ì‹œì‘")
    
    try:
        summaries = state.get("summaries", [])
        
        if not summaries:
            raise ValueError("í¬ë§·í•  ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ë§ˆí¬ë‹¤ìš´ í—¤ë” ìƒì„±
        timestamp = datetime.now()
        markdown_content = f"""# ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìš”ì•½

> ìƒì„± ì‹œê°: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}

"""
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½ ì¶”ê°€
        for summary_data in summaries:
            category = summary_data.get("category", "ì•Œ ìˆ˜ ì—†ìŒ")
            summary_text = summary_data.get("summary", "")
            article_count = summary_data.get("article_count", 0)
            success = summary_data.get("success", False)
            
            # ì¹´í…Œê³ ë¦¬ ì„¹ì…˜ í—¤ë”
            from ..models.schemas import get_category_emoji
            emoji = get_category_emoji(category)
            markdown_content += f"## {emoji} {category}\n\n"
            
            if success and summary_text:
                markdown_content += summary_text + "\n\n"
            else:
                markdown_content += f"âš ï¸ ì´ ì¹´í…Œê³ ë¦¬ì˜ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ({article_count}ê°œ ê¸°ì‚¬)\n\n"
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        total_articles = state.get("total_articles_scraped", 0)
        scraping_duration = state.get("scraping_duration", 0)
        summarization_duration = state.get("summarization_duration", 0)
        
        markdown_content += f"""---

## ğŸ“Š ìƒì„± ì •ë³´

- **ì´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜**: {total_articles}ê°œ
- **ìŠ¤í¬ë˜í•‘ ì†Œìš” ì‹œê°„**: {scraping_duration:.2f}ì´ˆ
- **ìš”ì•½ ì†Œìš” ì‹œê°„**: {summarization_duration:.2f}ì´ˆ
- **ì´ ì²˜ë¦¬ ì‹œê°„**: {scraping_duration + summarization_duration:.2f}ì´ˆ

---

*ì´ ë¦¬í¬íŠ¸ëŠ” ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìš”ì•½ ì—ì´ì „íŠ¸ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
        
        duration = time.time() - start_time
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        updated_state = state.copy()
        updated_state["final_markdown"] = markdown_content
        updated_state["formatting_duration"] = duration
        
        logger.info(f"í¬ë§·í„° ë…¸ë“œ ì™„ë£Œ: {len(markdown_content)}ì, {duration:.2f}ì´ˆ")
        return updated_state
        
    except Exception as e:
        duration = time.time() - start_time
        error_msg = f"í¬ë§·í„° ë…¸ë“œ ì˜¤ë¥˜: {str(e)}"
        logger.error(error_msg)
        
        updated_state = state.copy()
        updated_state["errors"].append(error_msg)
        updated_state["final_markdown"] = "# ì˜¤ë¥˜ ë°œìƒ\n\në§ˆí¬ë‹¤ìš´ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        updated_state["formatting_duration"] = duration
        
        return updated_state


class NewsAgent:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìš”ì•½ ì—ì´ì „íŠ¸
    
    LangGraph 0.6+ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ìˆ˜ì§‘ë¶€í„° ìš”ì•½ê¹Œì§€ì˜ 
    ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        checkpointer: Optional[str] = None,
        enable_streaming: bool = True,
        debug: bool = False
    ):
        """
        ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            checkpointer: ì²´í¬í¬ì¸í„° ìœ í˜• ("memory", "sqlite", None)
            enable_streaming: ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™” ì—¬ë¶€
            debug: ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
        """
        self.enable_streaming = enable_streaming
        self.debug = debug
        
        # ì²´í¬í¬ì¸í„° ì„¤ì •
        if checkpointer == "sqlite" and SqliteSaver is not None:
            self.checkpointer = SqliteSaver.from_conn_string("news_agent.db")
            logger.info("SQLite ì²´í¬í¬ì¸í„° í™œì„±í™”")
        elif checkpointer == "memory":
            self.checkpointer = MemorySaver()
            logger.info("ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° í™œì„±í™”")
        else:
            self.checkpointer = None
            logger.info("ì²´í¬í¬ì¸í„° ë¹„í™œì„±í™”")
        
        # ê·¸ë˜í”„ ë¹Œë“œ
        self.graph = self._build_graph()
        logger.info("ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _build_graph(self) -> StateGraph:
        """
        LangGraph StateGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        
        Returns:
            ì»´íŒŒì¼ëœ StateGraph
        """
        logger.info("StateGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì‹œì‘")
        
        # StateGraph ìƒì„±
        workflow = StateGraph(NewsAgentState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("scraper", scraper_node)
        workflow.add_node("retry_scraper", retry_scraper_node)
        workflow.add_node("summarizer", summarizer_node)
        workflow.add_node("retry_summarizer", retry_summarizer_node)
        workflow.add_node("formatter", formatter_node)
        
        # ê¸°ë³¸ í”Œë¡œìš° ì—£ì§€
        workflow.add_edge(START, "scraper")
        
        # ì¡°ê±´ë¶€ ì—£ì§€ - ìŠ¤í¬ë˜í•‘ í›„
        workflow.add_conditional_edges(
            "scraper",
            should_retry_scraping,
            {
                "retry_scraper": "retry_scraper",
                "summarizer": "summarizer"
            }
        )
        
        # ì¬ì‹œë„ ìŠ¤í¬ë˜í¼ì—ì„œ ìš”ì•½ìœ¼ë¡œ
        workflow.add_edge("retry_scraper", "summarizer")
        
        # ì¡°ê±´ë¶€ ì—£ì§€ - ìš”ì•½ í›„
        workflow.add_conditional_edges(
            "summarizer",
            should_retry_summarization,
            {
                "retry_summarizer": "retry_summarizer",
                "formatter": "formatter"
            }
        )
        
        # ì¬ì‹œë„ ìš”ì•½ì—ì„œ í¬ë§·í„°ë¡œ
        workflow.add_edge("retry_summarizer", "formatter")
        
        # í¬ë§·í„°ì—ì„œ ëìœ¼ë¡œ
        workflow.add_edge("formatter", END)
        
        # ê·¸ë˜í”„ ì»´íŒŒì¼
        compiled_graph = workflow.compile(
            checkpointer=self.checkpointer,
            debug=self.debug
        )
        
        logger.info("StateGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì™„ë£Œ")
        return compiled_graph

    def _get_thread_config(self, thread_id: Optional[str] = None) -> Dict:
        """
        ìŠ¤ë ˆë“œ ì„¤ì • ìƒì„±
        
        Args:
            thread_id: ìŠ¤ë ˆë“œ ID (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            
        Returns:
            ìŠ¤ë ˆë“œ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        if thread_id is None:
            thread_id = f"news_thread_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        return {"configurable": {"thread_id": thread_id}}

    async def run(
        self,
        categories: Optional[list] = None,
        thread_id: Optional[str] = None
    ) -> NewsAgentState:
        """
        ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì‹¤í–‰
        
        Args:
            categories: ì²˜ë¦¬í•  ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ê¸°ë³¸ê°’: ì „ì²´)
            thread_id: ìŠ¤ë ˆë“œ ID (ì²´í¬í¬ì¸íŒ…ìš©)
            
        Returns:
            ìµœì¢… ì‹¤í–‰ ê²°ê³¼
            
        Raises:
            GraphError: ê·¸ë˜í”„ ì‹¤í–‰ ì˜¤ë¥˜
        """
        start_time = datetime.now()
        logger.info(f"ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘: {start_time}")
        
        try:
            # ì´ˆê¸° ìƒíƒœ ìƒì„±
            initial_state = create_initial_state(categories)
            
            # ìŠ¤ë ˆë“œ ì„¤ì •
            config = self._get_thread_config(thread_id)
            
            # ê·¸ë˜í”„ ì‹¤í–‰
            if self.enable_streaming:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                final_state = None
                async for chunk in self.graph.astream(initial_state, config):
                    if self.debug:
                        logger.debug(f"ìŠ¤íŠ¸ë¦¼ ì²­í¬: {chunk.keys()}")
                    final_state = chunk
                
                if final_state is None:
                    raise GraphError("ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ì—ì„œ ê²°ê³¼ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                
                # ë§ˆì§€ë§‰ ì²­í¬ì—ì„œ ì‹¤ì œ ìƒíƒœ ì¶”ì¶œ
                result = list(final_state.values())[0]
                
            else:
                # ì¼ë°˜ ëª¨ë“œ
                result = await self.graph.ainvoke(initial_state, config)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info(f"ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ: {duration:.2f}ì´ˆ")
            
            # ê²°ê³¼ ê²€ì¦
            if not isinstance(result, dict):
                raise GraphError(f"ì˜ˆìƒí•˜ì§€ ëª»í•œ ê²°ê³¼ íƒ€ì…: {type(result)}")
            
            return result
            
        except Exception as e:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            error_msg = f"ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨ ({duration:.2f}ì´ˆ): {str(e)}"
            logger.error(error_msg)
            raise GraphError(error_msg)

    async def get_final_markdown(
        self,
        categories: Optional[list] = None,
        thread_id: Optional[str] = None
    ) -> str:
        """
        ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„± (í¸ì˜ ë©”ì„œë“œ)
        
        Args:
            categories: ì²˜ë¦¬í•  ì¹´í…Œê³ ë¦¬ ëª©ë¡
            thread_id: ìŠ¤ë ˆë“œ ID
            
        Returns:
            ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
        """
        result = await self.run(categories, thread_id)
        return result.get("final_markdown", "# ì˜¤ë¥˜\n\nê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def get_graph_visualization(self) -> str:
        """
        ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì‹œê°í™”
        
        Returns:
            ê·¸ë˜í”„ êµ¬ì¡° í…ìŠ¤íŠ¸
        """
        return """
ë„¤ì´ë²„ ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°:

START
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   scraper   â”‚ â”€â”€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
  â”‚                                â”‚
  â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ should_retry_       â”‚      â”‚ retry_scraper   â”‚
â”‚ scraping           â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
  â”‚                                  â”‚
  â–¼                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ summarizer  â”‚ â”€â”€ ìš”ì•½ ì‹¤íŒ¨ ì‹œ â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
  â”‚                             â”‚
  â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ should_retry_       â”‚   â”‚ retry_summarizer    â”‚
â”‚ summarization      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
  â”‚                               â”‚
  â–¼                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  formatter  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
END

ì£¼ìš” íŠ¹ì§•:
- ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ ìµœëŒ€ 2íšŒ ì¬ì‹œë„
- ìš”ì•½ ì‹¤íŒ¨ ì‹œ ìµœëŒ€ 1íšŒ ì¬ì‹œë„
- ê° ë‹¨ê³„ë³„ ìƒíƒœ ì˜ì†ì„± ì§€ì›
- ì¤‘ê°„ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥
"""


# í¸ì˜ í•¨ìˆ˜ë“¤
async def create_news_agent(
    checkpointer: str = "memory",
    enable_streaming: bool = True,
    debug: bool = False
) -> NewsAgent:
    """
    ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Args:
        checkpointer: ì²´í¬í¬ì¸í„° ìœ í˜•
        enable_streaming: ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        debug: ë””ë²„ê·¸ ëª¨ë“œ
        
    Returns:
        NewsAgent ì¸ìŠ¤í„´ìŠ¤
    """
    return NewsAgent(
        checkpointer=checkpointer,
        enable_streaming=enable_streaming,
        debug=debug
    )


async def generate_news_summary(
    categories: Optional[list] = None,
    thread_id: Optional[str] = None
) -> str:
    """
    ë‰´ìŠ¤ ìš”ì•½ ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Args:
        categories: ì¹´í…Œê³ ë¦¬ ëª©ë¡
        thread_id: ìŠ¤ë ˆë“œ ID
        
    Returns:
        ë§ˆí¬ë‹¤ìš´ ìš”ì•½ í…ìŠ¤íŠ¸
    """
    agent = await create_news_agent()
    return await agent.get_final_markdown(categories, thread_id)